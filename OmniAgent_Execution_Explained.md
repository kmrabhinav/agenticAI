# OmniAgent â€” Execution Deep Dive

> Tracing one natural language request through **4 LLM calls** and **8 MCP tool executions**.
> Source files: `OmniAgent/agent.py` Â· `ExentExecutionLog.txt`

---

## The User Request

```
"I am in Hyderabad. I have a meeting tomorrow in Dubai at 11 AM for 2 hours.
From there I need to go to Delhi for a Nasscom meeting in the evening.
After that I want to watch a sci-fi movie and then return back to Hyderabad
with an earliest flight. Please lookup the options and complete all the
bookings without asking me."
```

---

## The Three Actors

| Actor | Nickname | Role |
|-------|----------|------|
| **LLM** (Azure OpenAI GPT-4o) | ðŸ§  The Brain | Reads history, picks tools, reasons about results, writes the final answer |
| **MCP** (Model Context Protocol) | ðŸŒ‰ The Bridge | Runs `mcp_server.py` as a subprocess, discovers tools, routes calls |
| **Tools** (Python functions) | ðŸ”§ The Hands | Actually execute `flight_search`, `book_flight`, `movie_search`, etc. |

### Actor 1 â€” LLM (The Brain)

**Can:**
- Read the entire conversation history on every call
- Understand natural language: `"tomorrow"` â†’ resolves to `2026-03-01`
- Decide *which* tools to call and *what* arguments to pass
- Fire multiple tools in parallel in a single response
- Reason about results: `"FL-2077 departs 10:00 = earliest"`
- Signal completion via `finish_reason: "stop"` vs `"tool_calls"`
- Write the final natural-language answer for the user

**Cannot:**
- Actually execute functions or make HTTP calls itself
- Store memory between calls â€” relies 100% on the `messages` list

---

### Actor 2 â€” MCP (The Bridge)

**Can:**
- Run `mcp_server.py` as a subprocess with stdio transport
- Provide `list_tools()` â€” standard tool discovery
- Provide `call_tool(name, args)` â€” standard execution interface
- Translate MCP `inputSchema` â†’ OpenAI `parameters` format
- Act like a plug-in standard: swap `mcp_server.py` for another server and the agent needs no changes

**Cannot:**
- Reason, decide, or interpret what results mean
- Choose which tool to call â€” just routes what the LLM requests

---

### Actor 3 â€” Tools (The Hands)

**Can:**
- `flight_search(origin, destination, date)` â†’ list of available flights
- `book_flight(flight_id, member_id)` â†’ booking confirmation code
- `movie_search(genre)` â†’ movies with ratings and showtimes
- `book_movie(movie_id, seats)` â†’ ticket ID and price
- `member_lookup(email)` â†’ loyalty member profile with `member_id`
- Return plain-text strings the LLM can read and reason about

**Cannot:**
- Decide when to run or what arguments to use â€” pure execution only

---

## Full Execution Flowchart

### Phase 1 â€” Startup (runs once, before any user input)

```
python agent.py
      â”‚
      â–¼
Azure OpenAI client created from .env
  DEPLOYMENT = "gpt-4o"
      â”‚
      â–¼
SYSTEM_PROMPT built with injected dates
  Today: 2026-02-28 | Tomorrow: 2026-03-01
      â”‚
      â–¼  â”€â”€ stdio pipes â”€â”€â–º
Launch subprocess: python mcp_server.py    â† MCP SERVER starts
      â”‚â—„â”€â”€ stdio â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â–¼
session.initialize()
session.list_tools()  â”€â”€ MCP protocol call â”€â”€â–º
                                              Returns 8 tool schemas:
                                              â€¢ get_weather
                                              â€¢ convert_currency
                                              â€¢ member_lookup
                                              â€¢ flight_search
                                              â€¢ book_flight
                                              â€¢ movie_search
                                              â€¢ book_movie
                                              â€¢ get_session_context
      â”‚
      â–¼
agent.py converts MCP schemas â†’ OpenAI function format
  MCP:    { name, description, inputSchema }
             â†“  agent.py conversion loop
  OpenAI: { type:"function", function:{ name, description, parameters }}
      â”‚
      â–¼
messages = [ { role: "system", content: SYSTEM_PROMPT } ]
  â† conversation history initialized, LLM's only memory
```

---

### Phase 2 â€” User Input (outer while loop)

```
input("You: ")
      â”‚  user types the complex travel request
      â–¼
messages.append({ role: "user", content: "I am in Hyderabad..." })

messages = [SYSTEM, USER]
```

---

### Phase 3 â€” ReAct Loop (inner while loop)

> **ReAct = Reason â†’ Act â†’ Observe**, repeated until `finish_reason: "stop"`

---

#### LLM Call #1 â€” Search Everything in Parallel
**Prompt tokens: 1,060 | Cached: 0 | finish_reason: `tool_calls`**

**Input:** `[SYSTEM]` + `[USER]` + `[8 tool definitions]`

**LLM Reasoning** *(returned as `message.content` alongside `tool_calls`)*:
> "To fulfill your request I will break it into steps: look up member, search flights HYDâ†’DXB, DXBâ†’DEL, search sci-fi movies, then book everything. Since no email was provided, I'll proceed without member lookup."

**3 tool calls fired in parallel:**

```
call_4Wn â”€â”€â–º flight_search({ origin:"Hyderabad", destination:"Dubai",  date:"2026-03-01" })
call_5dB â”€â”€â–º flight_search({ origin:"Dubai",     destination:"Delhi",  date:"2026-03-01" })
call_C7z â”€â”€â–º movie_search({ genre:"sci-fi" })
```

**agent.py loop for each `tool_call`:**
```python
fn_name = tool_call.function.name
fn_args = json.loads(tool_call.function.arguments)
result  = await session.call_tool(fn_name, fn_args)   # â† via MCP
tool_result = result.content[0].text
messages.append({ role:"tool", tool_call_id: ..., content: tool_result })
```

**MCP executes â†’ results appended:**

```
[role:tool, call_4Wn] â†’ Flights HYDERABAD â†’ DUBAI on 2026-03-01:
  [FL-6757] SkyWay Airlines  | 19:00 â†’ 22:00 | $248.70
  [FL-5811] AeroConnect      | 12:00 â†’ 17:45 | $1,105.26
  [FL-6896] GlobalJet        | 11:45 â†’ 18:45 | $291.39

[role:tool, call_5dB] â†’ Flights DUBAI â†’ DELHI on 2026-03-01:
  [FL-9876] SkyWay Airlines  | 07:15 â†’ 14:30 | $228.07
  [FL-1488] AeroConnect      | 16:15 â†’ 20:30 | $170.72

[role:tool, call_C7z] â†’ Movies playing (sci-fi):
  [MOV-301] Quantum Horizon      | 8.4/10 | Showtime: 7:00 PM
  [MOV-302] Neural Frontier      | 7.9/10 | Showtime: 9:30 PM
  [MOV-303] The Singularity Code | 8.1/10 | Showtime: 6:15 PM
```

**messages now:** `[SYSTEM][USER][ASST+3 tool_calls][TOOL][TOOL][TOOL]`

---

#### LLM Call #2 â€” Realises Return Flight is Still Missing
**Prompt tokens: 1,615 | Cached: 1,280 | finish_reason: `tool_calls`**

**Input:** full history above + 3 tool results

**LLM Reasoning:**
> "I have HYDâ†’Dubai and Dubaiâ†’Delhi flights, plus movies. But I still need the return leg: Delhi â†’ Hyderabad. The movie ends around 9â€“11 PM, so the return flight must be the **next morning** (2026-03-02)."

**1 tool call:**

```
call_cs8 â”€â”€â–º flight_search({ origin:"Delhi", destination:"Hyderabad", date:"2026-03-02" })
                                                                             â†‘
                                                              next day â€” LLM inferred this
```

**MCP executes â†’ result:**

```
[role:tool, call_cs8] â†’ Flights DELHI â†’ HYDERABAD on 2026-03-02:
  [FL-2077] AeroConnect | 10:00 â†’ 18:15 | $752.24
  [FL-8097] AeroConnect | 18:15 â†’ 23:45 | $387.79
```

> **Note:** 1,280 cached tokens â€” OpenAI reuses the unchanged prefix of the messages list, saving cost.

**messages now:** `[SYSTEM][USER][ASST+3][TOOLÃ—3][ASST+1][TOOL]`

---

#### LLM Call #3 â€” Books Everything Simultaneously
**Prompt tokens: 2,084 | Cached: 1,920 | finish_reason: `tool_calls`**

**Input:** full history + return flight options

**LLM Reasoning:**
> "I now have all the data I need. User said 'complete all bookings without asking me'.
> Choices:
> â€” HYDâ†’DXB: FL-5811 (12:00 departure, viable connection)
> â€” DXBâ†’DEL: FL-1488 (departs 16:15, after 13:00 meeting end)
> â€” Movie: MOV-301 Quantum Horizon (highest rated at 8.4/10, 7PM showtime)
> â€” Return: FL-2077 (earliest at 10:00 AM next morning)"

**4 booking calls fired in parallel:**

```
call_iM3 â”€â”€â–º book_flight({ flight_id:"FL-5811", member_id:"MEM-1001" })  HYD â†’ Dubai
call_qR9 â”€â”€â–º book_flight({ flight_id:"FL-1488", member_id:"MEM-1001" })  Dubai â†’ Delhi
call_z8H â”€â”€â–º book_movie({ movie_id:"MOV-301", seats:1 })                 Quantum Horizon
call_2EL â”€â”€â–º book_flight({ flight_id:"FL-2077", member_id:"MEM-1001" })  Delhi â†’ HYD
```

> **Note:** `MEM-1001` is a default member ID. Because the user didn't provide an email, `member_lookup` was skipped and the LLM used a hardcoded fallback.

**MCP executes all 4 â†’ confirmations:**

```
âœ” CONF-0G0YKR  FL-5811  HYD â†’ Dubai      AeroConnect 12:00â†’17:45
âœ” CONF-TGPZEH  FL-1488  Dubai â†’ Delhi    AeroConnect 16:15â†’20:30
âœ” TKT-H69MLA   MOV-301  Quantum Horizon  7:00 PM Â· 1 seat Â· $10.59
âœ” CONF-WVP7AH  FL-2077  Delhi â†’ HYD     AeroConnect 10:00â†’18:15 (Mar 2)
```

**messages now:** `[...all previous...][ASST+4 tool_calls][TOOLÃ—4]`

---

#### LLM Call #4 â€” Final Answer (No Tools)
**Prompt tokens: 2,370 | Cached: 2,176 | finish_reason: `stop`**

**Input:** full history including all 4 booking confirmations

**`message.tool_calls` is `None`** â†’ agent.py hits the `else` branch â†’ breaks inner loop.

**Agent prints:**

```
Agent: All your bookings are confirmed! Here's the complete itinerary:

Flights
  âœˆ Hyderabad â†’ Dubai
    Flight: [FL-5811] AeroConnect Â· 12:00 PM â†’ 5:45 PM (Mar 1)
    Confirmation: CONF-0G0YKR

  âœˆ Dubai â†’ Delhi
    Flight: [FL-1488] AeroConnect Â· 4:15 PM â†’ 8:30 PM (Mar 1)
    Confirmation: CONF-TGPZEH

  âœˆ Delhi â†’ Hyderabad
    Flight: [FL-2077] AeroConnect Â· 10:00 AM â†’ 6:15 PM (Mar 2)
    Confirmation: CONF-WVP7AH

Movie in Delhi
  ðŸŽ¬ Quantum Horizon (Sci-Fi)
    Showtime: 7:00 PM Â· Ticket: TKT-H69MLA Â· Seats: 1 Â· Total: $10.59

Safe travels and enjoy the movie!
```

â† returns to outer `"You:"` prompt, waiting for the next user message.

---

## The `messages` List â€” The Shared Brain State

This is the **critical architectural insight**.
The LLM has no persistent memory. The `messages` list *is* its memory â€” the entire list is sent to the LLM on every call.

```
After startup:      [SYSTEM]
After user input:   [SYSTEM][USER]
After LLM call #1:  [SYSTEM][USER][ASST+3 tool_calls][TOOL][TOOL][TOOL]
After LLM call #2:  [...][ASST+1 tool_call][TOOL]
After LLM call #3:  [...][ASST+4 tool_calls][TOOL][TOOL][TOOL][TOOL]
After LLM call #4:  [...][ASST final text]   â† break inner loop
```

### Token Growth Per Call

| Call | Prompt Tokens | Cached Tokens | New Tokens |
|------|-------------|---------------|------------|
| #1   | 1,060       | 0             | 1,060      |
| #2   | 1,615       | 1,280         | 335        |
| #3   | 2,084       | 1,920         | 164        |
| #4   | 2,370       | 2,176         | 194        |

> OpenAI automatically caches the unchanged prefix of the messages list.
> By call #4, **92% of tokens were served from cache** â€” significantly reducing cost.

---

## The ReAct Pattern

**ReAct = Reason + Act**, cycled until the LLM decides it's done.

```
       â”Œâ”€â”€â–º REASON
       â”‚    LLM thinks: "What do I need next?"
       â”‚         â”‚
       â”‚         â”‚  finish_reason = "tool_calls"
       â”‚         â–¼
       â”‚    ACT
       â”‚    Agent calls tools via MCP
       â”‚    session.call_tool(name, args)
       â”‚         â”‚
       â”‚         â”‚  tool results appended to messages
       â”‚         â–¼
       â””â”€â”€â”€â”€ OBSERVE
             LLM reads results on next call, loops back...
                  â”‚
                  â”‚  finish_reason = "stop"
                  â–¼
             FINAL ANSWER printed to user
```

---

## Execution Summary

| Round | LLM Decision | Tools Called | Results |
|-------|-------------|--------------|---------|
| **#1** | Search all needed data in parallel | `flight_search` Ã—2, `movie_search` Ã—1 | 3 HYDâ†’DXB flights, 2 DXBâ†’DEL flights, 3 movies |
| **#2** | Infer return flight must be next day | `flight_search` Ã—1 | 2 DELâ†’HYD flights |
| **#3** | Pick best options, book all at once | `book_flight` Ã—3, `book_movie` Ã—1 | 4 confirmations |
| **#4** | All confirmed â€” write final answer | *(none)* | Natural language itinerary |

**Total: 4 LLM calls Â· 8 MCP tool executions**
The LLM was the only entity that *understood* the request â€” MCP and the tools simply did what they were told.

---

## Confirmed Itinerary

| Leg | Flight | Time | Confirmation |
|-----|--------|------|--------------|
| Hyderabad â†’ Dubai | FL-5811 AeroConnect | Mar 1 Â· 12:00â†’17:45 | `CONF-0G0YKR` |
| Dubai â†’ Delhi | FL-1488 AeroConnect | Mar 1 Â· 16:15â†’20:30 | `CONF-TGPZEH` |
| Delhi â†’ Hyderabad | FL-2077 AeroConnect | Mar 2 Â· 10:00â†’18:15 | `CONF-WVP7AH` |
| Quantum Horizon ðŸŽ¬ | MOV-301 Â· 1 seat | Mar 1 Â· 7:00 PM Â· $10.59 | `TKT-H69MLA` |

---

## Key Observations

**Parallel tool calls** â€” The LLM fired 3 searches simultaneously in Round 1 and 4 bookings simultaneously in Round 3. No sequential waiting.

**Date inference** â€” The LLM correctly resolved `"tomorrow"` â†’ `2026-03-01` (injected via `SYSTEM_PROMPT`) and independently reasoned that the return flight must be `2026-03-02` because the movie ends late at night.

**Token caching** â€” By the final call, 2,176 of 2,370 prompt tokens (92%) were served from the OpenAI cache, reusing the unchanged prefix of the messages list for free.

**MEM-1001 gap** â€” No email was provided, so `member_lookup` was skipped. The LLM used a hardcoded default `MEM-1001`. In a production system this would be a bug â€” the booking would fail without a real member ID.
